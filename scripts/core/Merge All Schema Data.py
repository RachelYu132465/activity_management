# scripts/actions/merge_all_schema_data.py
# ç›®çš„ï¼šä¸å¸¶ä»»ä½•æŒ‡ä»¤åƒæ•¸ï¼Œç›´æ¥è‡ªå‹•æƒæ config/schema/ ä¸‹çš„æ‰€æœ‰ schemaï¼Œ
# åœ¨ data/ å…§å°‹æ‰¾å°æ‡‰è³‡æ–™æª”ï¼ˆ*_data.json / *.json / *.csvï¼‰ï¼Œ
# ä»¥ schema çš„é è¨­éµå€¼è£œé½Šæ¯ä¸€åˆ—è³‡æ–™ï¼Œä¸¦ä¿ç•™ data çš„é¡å¤–æ¬„ä½ï¼ˆä¸ä¸Ÿï¼‰ã€‚
# ç”¢å‡ºå¯«åˆ° output/merged/<name>_filled.jsonï¼Œä¸¦ç”Ÿæˆåˆä½µå ±å‘Šã€‚
# åŠ å¼·ç‰ˆï¼š
#  - åš´æ ¼é©—è­‰ä¸¦åœ¨ã€Œæ¬„ä½å‹åˆ¥ä¸ç¬¦ã€è³‡æ–™å½¢ç‹€ä¸ç¬¦ã€æ™‚ç«‹åˆ»åœæ­¢åŸ·è¡Œï¼Œè¼¸å‡ºéŒ¯èª¤å ±å‘Šï¼ˆå«æ¬„ä½åèˆ‡åˆ—è™Ÿï¼‰ã€‚

from __future__ import annotations
import json, csv, sys
from pathlib import Path
from typing import Any, Dict, List, Iterable, Tuple
from datetime import datetime

# === å…¨åŸŸè¨­å®š ===
STRICT_STOP_ON_ERROR = True          # åµæ¸¬åˆ°éŒ¯èª¤ï¼ˆå¦‚å‹åˆ¥ä¸ç¬¦ï¼‰æ™‚ï¼Œç«‹åˆ»åœæ­¢æ•´å€‹æµç¨‹
FILL_EMPTY_DEFAULT = False           # é è¨­ä¸ä»¥ default è¦†å¯«ç©ºå€¼
CARRY_EXTRA_DEFAULT = True           # é è¨­ä¿ç•™ data é¡å¤–æ¬„ä½
ALLOW_NUMERIC_COMPAT = True          # å…è¨± int/float äº’é€š
ALLOW_NUMERIC_STRING = False         # ä¸æŠŠæ•¸å­—å­—ä¸²è¦–ç‚ºåˆæ³•

# --- è·¯å¾‘è¨­å®š ---
BASE_DIR = Path(__file__).resolve().parents[2]
DATA_DIR = BASE_DIR / "data"
SCHEMA_DIR = BASE_DIR / "config" / "schema"
OUTPUT_DIR = BASE_DIR / "output"
MERGED_DIR = OUTPUT_DIR / "merged"
REPORTS_DIR = OUTPUT_DIR / "reports"

# --- åˆå§‹åŒ– ---
def initialize() -> None:
    for p in (OUTPUT_DIR, MERGED_DIR, REPORTS_DIR):
        p.mkdir(parents=True, exist_ok=True)

# --- æª”æ¡ˆæœå°‹ ---
def rglob_one(base: Path, name: str) -> Path | None:
    for p in base.rglob("*"):
        if p.name == name:
            return p
    return None

PAIRING_RULES: Dict[str, List[str]] = {
    "program": ["program_data.json", "programs.json", "program.json", "programs.csv", "program.csv"],
    "influencer": ["influencer_data.json", "influencers.json", "influencer.json", "influencers.csv", "influencer.csv"],
    "follower": ["follower_data.json", "followers.json", "follower.json", "followers.csv", "follower.csv"],
    "event_contacts": ["event_contacts.json", "event_contacts_data.json", "event_contacts.csv"],
}
PATTERNS = [
    "{b}_data.json", "{b}.json", "{b}s.json", "{b}_data.csv", "{b}.csv", "{b}s.csv"
]

# --- è¼‰å…¥å™¨ ---
def load_json_any(path: Path) -> Any:
    return json.loads(path.read_text(encoding="utf-8"))

def load_json_as_rows(path: Path) -> List[Dict[str, Any]]:
    obj = load_json_any(path)
    if isinstance(obj, list):
        if obj and not isinstance(obj[0], dict):
            raise ValueError(f"JSON array å¿…é ˆæ˜¯ç‰©ä»¶é™£åˆ—: {path}")
        return obj
    if isinstance(obj, dict):
        return [obj]
    raise ValueError(f"ä¸æ”¯æ´çš„ JSON å½¢ç‹€: {path}")

def load_csv_as_rows(path: Path) -> List[Dict[str, Any]]:
    with path.open(newline="", encoding="utf-8") as f:
        return list(csv.DictReader(f))

def load_rows_by_ext(path: Path) -> List[Dict[str, Any]]:
    if path.suffix.lower() == ".csv":
        return load_csv_as_rows(path)
    return load_json_as_rows(path)

# --- å°å·¥å…· ---
def is_empty(v: Any) -> bool:
    return v is None or v == "" or v == [] or v == {}

def to_json(obj: Any) -> str:
    return json.dumps(obj, ensure_ascii=False, indent=2)

# å‹åˆ¥ç›¸å®¹æ€§
NUMERIC_TYPES = (int, float)

def same_type_or_compatible(default_val: Any, data_val: Any) -> bool:
    td, tv = type(default_val), type(data_val)
    if td is tv:
        return True
    if ALLOW_NUMERIC_COMPAT and (td in NUMERIC_TYPES) and (tv in NUMERIC_TYPES):
        return True
    if ALLOW_NUMERIC_STRING and (td in NUMERIC_TYPES) and isinstance(data_val, str):
        s = data_val.strip()
        if s.replace("_", "").replace(",", "").replace(".", "", 1).lstrip("+-").isdigit():
            return True
    return False

# --- é©—è­‰éŒ¯èª¤é¡å‹ ---
class MergeValidationError(Exception):
    pass

# --- åˆä½µèˆ‡é©—è­‰ ---
def merge_row(schema_defaults: Dict[str, Any], row: Dict[str, Any],
              fill_empty: bool = FILL_EMPTY_DEFAULT,
              carry_extra: bool = CARRY_EXTRA_DEFAULT,
              row_index: int | None = None,
              schema_name: str | None = None,
              data_rel: str | None = None,
              errors: List[str] | None = None) -> Dict[str, Any]:
    if errors is None:
        errors = []
    if not isinstance(row, dict):
        errors.append(f"E000 éç‰©ä»¶åˆ— row_index={row_index}")
        return row

    merged: Dict[str, Any] = {}
    for k, default in schema_defaults.items():
        if k in row:
            v = row[k]
            if not is_empty(v) and not same_type_or_compatible(default, v):
                dt, vt = type(default).__name__, type(v).__name__
                errors.append(
                    f"E001 å‹åˆ¥ä¸ç¬¦ field='{k}' row={row_index} expected={dt} got={vt} value={repr(v)}"
                )
            merged[k] = (default if (fill_empty and is_empty(v)) else v)
        else:
            merged[k] = default
    if carry_extra:
        for k, v in row.items():
            if k not in schema_defaults:
                merged[k] = v
    return merged

# --- åŒ¹é…è³‡æ–™æª” ---
def gen_candidates(base_name: str) -> List[Path]:
    candidates: List[Path] = []
    for name in PAIRING_RULES.get(base_name, []):
        p = rglob_one(DATA_DIR, name)
        if p:
            candidates.append(p)
    for pat in PATTERNS:
        name = pat.format(b=base_name)
        p = rglob_one(DATA_DIR, name)
        if p and p not in candidates:
            candidates.append(p)
    for p in DATA_DIR.rglob("*"):
        if p.is_file() and base_name in p.stem and p.suffix.lower() in {".json", ".csv"} and p not in candidates:
            candidates.append(p)
    return candidates

def score_candidate(path: Path, schema_keys: Iterable[str]) -> int:
    try:
        rows = load_rows_by_ext(path)
        if not rows:
            return 0
        sample_keys = set(rows[0].keys())
        return len(sample_keys.intersection(schema_keys))
    except Exception:
        return -1

def pick_best_candidate(base_name: str, schema: Dict[str, Any]) -> Path | None:
    cands = gen_candidates(base_name)
    if not cands:
        return None
    schema_keys = set(schema.keys())
    scored: List[Tuple[int, Path]] = [(score_candidate(p, schema_keys), p) for p in cands]
    scored.sort(key=lambda x: x[0], reverse=True)
    best = scored[0]
    return best[1] if best[0] >= 0 else None

# --- å ±å‘Šå·¥å…· ---
def write_error_report(schema_rel: str, data_rel: str | None, messages: List[str]) -> Path:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = REPORTS_DIR / f"merge_error_{ts}.md"
    lines = [
        f"# Merge Error Report ({ts})",
        f"schema: {schema_rel}",
        f"data:   {data_rel}",
        "",
        "**Errors:**",
    ]
    lines.extend([f"- {m}" for m in messages])
    report_path.write_text("\n".join(lines), encoding="utf-8")
    return report_path

# --- ä¸»æµç¨‹ ---
def merge_one_schema(schema_path: Path, fill_empty: bool = FILL_EMPTY_DEFAULT, carry_extra: bool = CARRY_EXTRA_DEFAULT) -> Dict[str, Any]:
    base = schema_path.stem
    schema = load_json_any(schema_path)
    if not isinstance(schema, dict):
        raise MergeValidationError(f"E100 Schema å¿…é ˆæ˜¯æ‰å¹³ dict: {schema_path.name}")

    data_path = pick_best_candidate(base, schema)
    result_info: Dict[str, Any] = {
        "schema": str(schema_path.relative_to(BASE_DIR)),
        "schema_keys": sorted(list(schema.keys())),
        "data": None,
        "output": None,
        "rows": 0,
        "carry_extra": carry_extra,
        "fill_empty": fill_empty,
    }

    if not data_path:
        out_path = MERGED_DIR / f"{base}_filled.json"
        rows = [schema]
        out_path.write_text(to_json(rows), encoding="utf-8")
        result_info.update({
            "data": None,
            "output": str(out_path.relative_to(BASE_DIR)),
            "rows": 1,
        })
        return result_info

    rows = load_rows_by_ext(data_path)
    merged: List[Dict[str, Any]] = []
    errors: List[str] = []

    for idx, r in enumerate(rows, start=1):
        if not isinstance(r, dict):
            errors.append(f"E000 éç‰©ä»¶åˆ— row_index={idx}")
            break
        m = merge_row(schema, r, fill_empty=fill_empty, carry_extra=carry_extra,
                      row_index=idx, schema_name=schema_path.name,
                      data_rel=str(data_path.relative_to(BASE_DIR)), errors=errors)
        merged.append(m)
        if STRICT_STOP_ON_ERROR and errors:
            break

    if errors:
        report = write_error_report(
            str(schema_path.relative_to(BASE_DIR)),
            str(data_path.relative_to(BASE_DIR)),
            errors,
        )
        print(f"âŒ é©—è­‰å¤±æ•—ï¼ˆå·²åœæ­¢ï¼‰ï¼š{schema_path.name} | å ±å‘Šï¼š{report}")
        raise MergeValidationError("; ".join(errors))

    out_path = MERGED_DIR / f"{base}_filled.json"
    out_path.write_text(to_json(merged), encoding="utf-8")

    result_info.update({
        "data": str(data_path.relative_to(BASE_DIR)),
        "output": str(out_path.relative_to(BASE_DIR)),
        "rows": len(rows),
    })
    return result_info

def run(fill_empty: bool = FILL_EMPTY_DEFAULT, carry_extra: bool = CARRY_EXTRA_DEFAULT) -> None:
    initialize()
    if not SCHEMA_DIR.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° schema ç›®éŒ„: {SCHEMA_DIR}")

    results: List[Dict[str, Any]] = []
    try:
        for schema_path in sorted(SCHEMA_DIR.glob("*.json")):
            info = merge_one_schema(schema_path, fill_empty=fill_empty, carry_extra=carry_extra)
            results.append(info)
            print(f"âœ… {schema_path.name} åˆä½µå®Œæˆ â†’ {info['output']}")
    except MergeValidationError as e:
        print(f"\nâ›” å·²åœæ­¢ï¼š{e}")
        sys.exit(2)

    # æˆåŠŸçš„æƒ…æ³å¯«ç¸½å ±å‘Š
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = REPORTS_DIR / f"merge_report_{ts}.md"
    lines: List[str] = [
        f"# Merge Report ({ts})",
        f"BASE_DIR: {BASE_DIR}",
        "",
    ]
    for info in results:
        lines.append(f"- {info['schema']} â†’ {info['output']} (rows={info['rows']})")
    report_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"ğŸ“„ å ±å‘Šï¼š{report_path}")

# --- å…¥å£é» ---
if __name__ == "__main__":
    run(fill_empty=FILL_EMPTY_DEFAULT, carry_extra=CARRY_EXTRA_DEFAULT)
